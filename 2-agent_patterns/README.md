# Agent Patterns    

## 1. `deterministic.py`   
The `deterministic.py` file demonstrates the [Prompt Chaining Workflow](https://www.anthropic.com/engineering/building-effective-agents) provided by Anthropic in 'Building Effective Agents blog.


## 2. `agent_as_tool`    
The `agent_as_tool` is another agentic pattern provided by openai-agents sdk. This agentic pattern is useful when you have an orchestrator agent or supervisor agent. It reflects the [Orchestrator-workers](https://www.anthropic.com/engineering/building-effective-agents) provided by Anthropic in 'Building Effective Agents blog.

But there is a slight difference. As per anthropic agent pattern of `orchestrator-worker`, The control is shifted to each worker and then the synthesizer add all the worker output. In the example provided by OpenAI, the synthesizer agent is not doing anything. Because the final output is already generated by the orchestrator agent. 

## 3. Forcing Tool use with `tool_use_behavior`   

This lets you configure how tool use is handled.

1. "run_llm_again": The default behavior. Tools are run, and then the LLM receives the results and gets to respond.

```json
Running agent Weather agent (turn 1)
Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x00000218712D5DF0>
[
  {
    "content": "You are a helpful agent.",
    "role": "system"
  },
  {
    "role": "user",
    "content": "What's the weather in Tokyo?"
  }
]
Tools:
[
  {
    "type": "function",
    "function": {
      "name": "get_weather",
      "description": "",
      "parameters": {
        "properties": {
          "city": {
            "title": "City",
            "type": "string"
          }
        },
        "required": [
          "city"
        ],
        "title": "get_weather_args",
        "type": "object",
        "additionalProperties": false
      }
    }
  }
]
Stream: False
Tool choice: NOT_GIVEN
Response format: NOT_GIVEN

LLM resp:
{
  "content": null,
  "refusal": null,
  "role": "assistant",
  "annotations": null,
  "audio": null,
  "function_call": null,
  "tool_calls": [
    {
      "id": "",
      "function": {
        "arguments": "{\"city\":\"Tokyo\"}",
        "name": "get_weather"
      },
      "type": "function"
    }
  ]
}

Tracing is disabled. Not creating span <agents.tracing.span_data.FunctionSpanData object at 0x00000218717900B0>
Invoking tool get_weather with input {"city":"Tokyo"}
Tool call args: ['Tokyo'], kwargs: {}
Weather tool called
Tool get_weather returned city='Tokyo' temperature_range='14 to 20 deg' conditions='sunny'
Running agent Weather agent (turn 2)
Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000021871790410> 
[
  {
    "content": "You are a helpful agent.",
    "role": "system"
  },
  {
    "role": "user",
    "content": "What's the weather in Tokyo?"
  },
  {
    "role": "assistant",
    "tool_calls": [
      {
        "id": "",
        "type": "function",
        "function": {
          "name": "get_weather",
          "arguments": "{\"city\":\"Tokyo\"}"
        }
      }
    ]
  },
  {
    "role": "tool",
    "tool_call_id": "",
    "content": "city='Tokyo' temperature_range='14 to 20 deg' conditions='sunny'"
  }
]
Tools:
[
  {
    "type": "function",
    "function": {
      "name": "get_weather",
      "description": "",
      "parameters": {
        "properties": {
          "city": {
            "title": "City",
            "type": "string"
          }
        },
        "required": [
          "city"
        ],
        "title": "get_weather_args",
        "type": "object",
        "additionalProperties": false
      }
    }
  }
]
Stream: False
Tool choice: NOT_GIVEN
Response format: NOT_GIVEN

LLM resp:
{
  "content": "The weather in Tokyo is sunny with a temperature range of 14 to 20 degrees.\n",
  "refusal": null,
  "role": "assistant",
  "annotations": null,
  "audio": null,
  "function_call": null,
  "tool_calls": null
}

Resetting current trace
The weather in Tokyo is sunny with a temperature range of 14 to 20 degrees.

```   

2. "stop_on_first_tool": The output of the first tool call is used as the final output. This means that the LLM does not process the result of the tool call.

```json
Running agent Weather agent (turn 1)
Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002399FCA5DF0>
[
  {
    "content": "You are a helpful agent.",
    "role": "system"
  },
  {
    "role": "user",
    "content": "What's the weather in Tokyo?"
  }
]
Tools:
[
  {
    "type": "function",
    "function": {
      "name": "get_weather",
      "description": "",
      "parameters": {
        "properties": {
          "city": {
            "title": "City",
            "type": "string"
          }
        },
        "required": [
          "city"
        ],
        "title": "get_weather_args",
        "type": "object",
        "additionalProperties": false
      }
    }
  }
]
Stream: False
Tool choice: NOT_GIVEN
Response format: NOT_GIVEN

LLM resp:
{
  "content": null,
  "refusal": null,
  "role": "assistant",
  "annotations": null,
  "audio": null,
  "function_call": null,
  "tool_calls": [
    {
      "id": "",
      "function": {
        "arguments": "{\"city\":\"Tokyo\"}",
        "name": "get_weather"
      },
      "type": "function"
    }
  ]
}

Tracing is disabled. Not creating span <agents.tracing.span_data.FunctionSpanData object at 0x00000239A01600B0>
Invoking tool get_weather with input {"city":"Tokyo"}
Tool call args: ['Tokyo'], kwargs: {}
Weather tool called
Tool get_weather returned city='Tokyo' temperature_range='14 to 20 deg' conditions='sunny'
Resetting current trace
city='Tokyo' temperature_range='14 to 20 deg' conditions='sunny'
```
3. A list of tool names: The agent will stop running if any of the tools in the list are called. The final output will be the output of the first matching tool call. The LLM does not process the result of the tool call.


4. A function: If you pass a function, it will be called with the run context and the list of tool results. It must return a ToolToFinalOutputResult, which determines whether the tool calls result in a final output.
NOTE: This configuration is specific to FunctionTools. Hosted tools, such as file search, web search, etc are always processed by the LLM.

```json
Running agent Weather agent (turn 1)
Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001C763605D90>
[
  {
    "content": "You are a helpful agent.",
    "role": "system"
  },
  {
    "role": "user",
    "content": "What's the weather in Tokyo?"
  }
]
Tools:
[
  {
    "type": "function",
    "function": {
      "name": "get_weather",
      "description": "",
      "parameters": {
        "properties": {
          "city": {
            "title": "City",
            "type": "string"
          }
        },
        "required": [
          "city"
        ],
        "title": "get_weather_args",
        "type": "object",
        "additionalProperties": false
      }
    }
  }
]
Stream: False
Tool choice: required
Response format: NOT_GIVEN

LLM resp:
{
  "content": null,
  "refusal": null,
  "role": "assistant",
  "annotations": null,
  "audio": null,
  "function_call": null,
  "tool_calls": [
    {
      "id": "",
      "function": {
        "arguments": "{\"city\":\"Tokyo\"}",
        "name": "get_weather"
      },
      "type": "function"
    }
  ]
}

Tracing is disabled. Not creating span <agents.tracing.span_data.FunctionSpanData object at 0x000001C763AC00B0>
Invoking tool get_weather with input {"city":"Tokyo"}
Tool call args: ['Tokyo'], kwargs: {}
Weather tool called
Tool get_weather returned city='Tokyo' temperature_range='14 to 20 deg' conditions='sunny'
Resetting current trace
Tokyo is sunny
```

### Best Practices   
1. Use "run_llm_again" when:
- You need the LLM to process and refine tool outputs
- Multiple tool calls might be needed
- Complex reasoning is required
2. Use "stop_on_first_tool" when:
- You want direct, unmodified tool outputs
- The tool result is final and doesn't need processing
- You want to save on API calls
3. Use custom behavior when:
- You need specific formatting of tool outputs
- You want to combine multiple tool results
- You need to implement custom logic for handling tool results

### Important Notes   
- The tool_use_behavior setting is specific to FunctionTools. Hosted tools (like file search, web search) are always processed by the LLM.
- When using "run_llm_again", be careful with tool_choice="required" as it might cause infinite loops if the LLM is forced to use a tool every time.
- Custom behavior functions should always return a ToolsToFinalOutputResult with:
  - is_final_output: Boolean indicating if this is the final output
  - final_output: The actual output string




```json

RawResponsesStreamEvent(
  data=ResponseCreatedEvent(
    response=Response(
      id='__fake_id__', created_at=1747998864.6930985, error=None, incomplete_details=None, instructions=None, metadata=None, model='gemini-2.0-flash', object='response', output=[], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, background=None, max_output_tokens=None, previous_response_id=None, reasoning=None, service_tier=None, status=None, text=None, truncation=None, usage=None, user=None
      ),
    sequence_number=0,
    type='response.created'
    ),
  type='raw_response_event'
)

```


```json
RawResponsesStreamEvent(
  data=ResponseOutputItemAddedEvent(
    item=ResponseOutputMessage(
      id='__fake_id__',
      content=[],
      role='assistant',
      status='in_progress',
      type='message'
    ),
    output_index=0,
    sequence_number=1,
    type='response.output_item.added'
  ),
  type='raw_response_event'
)
```
